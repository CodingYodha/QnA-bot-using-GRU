{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "qna bot gru 2",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/CodingYodha/QnA-bot-using-GRU.git"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-12T11:10:38.633941Z",
          "iopub.execute_input": "2025-03-12T11:10:38.63441Z",
          "iopub.status.idle": "2025-03-12T11:10:39.467886Z",
          "shell.execute_reply.started": "2025-03-12T11:10:38.634368Z",
          "shell.execute_reply": "2025-03-12T11:10:39.466798Z"
        },
        "id": "CIZyoO-n5A-D",
        "outputId": "8deac50e-5c8f-473f-820e-a252793364df"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Cloning into 'QnA-bot-using-GRU'...\nremote: Enumerating objects: 12, done.\u001b[K\nremote: Counting objects: 100% (12/12), done.\u001b[K\nremote: Compressing objects: 100% (10/10), done.\u001b[K\nremote: Total 12 (delta 1), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (12/12), 8.84 KiB | 2.95 MiB/s, done.\nResolving deltas: 100% (1/1), done.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-12T11:10:39.469329Z",
          "iopub.execute_input": "2025-03-12T11:10:39.469682Z",
          "iopub.status.idle": "2025-03-12T11:10:52.206543Z",
          "shell.execute_reply.started": "2025-03-12T11:10:39.469649Z",
          "shell.execute_reply": "2025-03-12T11:10:52.205786Z"
        },
        "id": "vXCMmBKs5A-D"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install convokit --no-deps\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-12T11:10:52.208582Z",
          "iopub.execute_input": "2025-03-12T11:10:52.209088Z",
          "iopub.status.idle": "2025-03-12T11:11:00.060593Z",
          "shell.execute_reply.started": "2025-03-12T11:10:52.209062Z",
          "shell.execute_reply": "2025-03-12T11:11:00.059691Z"
        },
        "id": "nDCiD3Bp5A-E",
        "outputId": "10be211f-0a57-4b7e-d3fd-d06215f1fdf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting convokit\n  Downloading convokit-3.1.0.tar.gz (193 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: convokit\n  Building wheel for convokit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for convokit: filename=convokit-3.1.0-py3-none-any.whl size=229062 sha256=ba6d8a648fa1b45472907488812b64e14546396e1b2b0b7345b4486b4cbdbe25\n  Stored in directory: /root/.cache/pip/wheels/3d/6c/a6/ff02f7442da7b87607faba9e518247c4db5d68f7ed39c2f610\nSuccessfully built convokit\nInstalling collected packages: convokit\nSuccessfully installed convokit-3.1.0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from convokit import Corpus, download\n",
        "\n",
        "# Download and load the corpus\n",
        "corpus = Corpus(filename=download(\"movie-corpus\"))\n",
        "corpus.print_summary_stats()\n",
        "\n",
        "# Extract utterance data\n",
        "data = []\n",
        "for utt in corpus.iter_utterances():\n",
        "    data.append({\n",
        "        \"conversation_id\": utt.conversation_id,\n",
        "        \"utterance_id\": utt.id,\n",
        "        \"speaker\": utt.speaker.id,\n",
        "        \"text\": utt.text\n",
        "    })\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-12T11:11:00.062097Z",
          "iopub.execute_input": "2025-03-12T11:11:00.062418Z",
          "iopub.status.idle": "2025-03-12T11:11:50.981925Z",
          "shell.execute_reply.started": "2025-03-12T11:11:00.062386Z",
          "shell.execute_reply": "2025-03-12T11:11:50.980928Z"
        },
        "id": "EDR0KSr_5A-E",
        "outputId": "75ecee67-f194-4c70-cfbc-049bddfb3001"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "An error occurred: No module named 'cleantext'\nNo configuration file found at /root/.convokit/config.yml; writing with contents: \n# Default Backend Parameters\ndb_host: localhost:27017\ndata_directory: ~/.convokit/saved-corpora\nmodel_directory: ~/.convokit/saved-models\ndefault_backend: mem\nDownloading movie-corpus to /root/.convokit/saved-corpora/movie-corpus\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/convokit/__init__.py:26: UserWarning: If you are using ConvoKit with Google Colab, incorrect versions of some packages (ex. scipy) may be imported while runtime start. To fix the issue, restart the session and run all codes again. Thank you!\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Downloading movie-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip (40.9MB)... Done\nNumber of Speakers: 9035\nNumber of Utterances: 304713\nNumber of Conversations: 83097\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Group utterances by conversation_id\n",
        "from collections import defaultdict\n",
        "\n",
        "conversations = defaultdict(list)\n",
        "for entry in data:\n",
        "    conversations[entry['conversation_id']].append((entry['utterance_id'], entry['speaker'], entry['text']))\n",
        "\n",
        "# Extract question-answer pairs\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "for conv_id, utterances in conversations.items():\n",
        "    for i in range(len(utterances) - 1):\n",
        "        current_utterance = utterances[i][2]  # Current utterance text\n",
        "        next_utterance = utterances[i + 1][2]  # Next utterance text\n",
        "        questions.append(current_utterance)\n",
        "        answers.append(next_utterance)\n",
        "\n",
        "# Example output\n",
        "print(\"Question:\", questions[100])\n",
        "print(\"Answer:\", answers[100])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-12T11:11:50.982852Z",
          "iopub.execute_input": "2025-03-12T11:11:50.983398Z",
          "iopub.status.idle": "2025-03-12T11:11:51.473122Z",
          "shell.execute_reply.started": "2025-03-12T11:11:50.983354Z",
          "shell.execute_reply": "2025-03-12T11:11:51.472257Z"
        },
        "id": "XemtUkhL5A-F",
        "outputId": "57f4402a-9441-4524-ff33-39727ddf867d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Question: What do you think?\nAnswer: Oh, I thought you might have a date  I don't know why I'm bothering to ask, but are you going to Bogey Lowenstein's party Saturday night?\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"\\'s\", \" is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z?.!]+\", \" \", text)\n",
        "    text = text.strip()\n",
        "    return text.lower()\n",
        "\n",
        "questions = [clean_text(q) for q in questions]\n",
        "answers = [clean_text(a) for a in answers]\n",
        "\n",
        "# Add <start> and <end> tokens to answers\n",
        "answers = ['<start> ' + a + ' <end>' for a in answers]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-12T11:11:51.474076Z",
          "iopub.execute_input": "2025-03-12T11:11:51.474385Z",
          "iopub.status.idle": "2025-03-12T11:11:54.916202Z",
          "shell.execute_reply.started": "2025-03-12T11:11:51.474355Z",
          "shell.execute_reply": "2025-03-12T11:11:54.915263Z"
        },
        "id": "GA36kZUY5A-G"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-12T11:11:54.91714Z",
          "iopub.execute_input": "2025-03-12T11:11:54.917455Z",
          "iopub.status.idle": "2025-03-12T11:11:54.984469Z",
          "shell.execute_reply.started": "2025-03-12T11:11:54.917431Z",
          "shell.execute_reply": "2025-03-12T11:11:54.983552Z"
        },
        "id": "5hQfpQ_F5A-G"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "tokenizer = Tokenizer(num_words=10000,filters=\"\", oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(questions + answers)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-12T11:11:54.985521Z",
          "iopub.execute_input": "2025-03-12T11:11:54.985885Z",
          "iopub.status.idle": "2025-03-12T11:11:59.563426Z",
          "shell.execute_reply.started": "2025-03-12T11:11:54.985852Z",
          "shell.execute_reply": "2025-03-12T11:11:59.562708Z"
        },
        "id": "7WN1sADZ5A-G"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulary size\n",
        "VOCAB_SIZE = 10000#len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-12T11:11:59.566049Z",
          "iopub.execute_input": "2025-03-12T11:11:59.566298Z",
          "iopub.status.idle": "2025-03-12T11:11:59.569505Z",
          "shell.execute_reply.started": "2025-03-12T11:11:59.566278Z",
          "shell.execute_reply": "2025-03-12T11:11:59.56873Z"
        },
        "id": "HhtFnmy45A-G"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to sequences\n",
        "question_sequences = tokenizer.texts_to_sequences(questions)\n",
        "answer_sequences = tokenizer.texts_to_sequences(answers)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-12T11:11:59.570467Z",
          "iopub.execute_input": "2025-03-12T11:11:59.570748Z",
          "iopub.status.idle": "2025-03-12T11:12:02.740876Z",
          "shell.execute_reply.started": "2025-03-12T11:11:59.570725Z",
          "shell.execute_reply": "2025-03-12T11:12:02.739965Z"
        },
        "id": "YMWMM1IY5A-H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad sequences\n",
        "MAX_LENGTH = 19  # Adjust based on your data\n",
        "question_padded = pad_sequences(question_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
        "answer_padded = pad_sequences(answer_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-12T11:12:02.741742Z",
          "iopub.execute_input": "2025-03-12T11:12:02.742064Z",
          "iopub.status.idle": "2025-03-12T11:12:03.779836Z",
          "shell.execute_reply.started": "2025-03-12T11:12:02.742033Z",
          "shell.execute_reply": "2025-03-12T11:12:03.778865Z"
        },
        "id": "WOzGZcX55A-H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import GRU, Dense, Embedding, Input, Attention\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-12T11:12:03.780718Z",
          "iopub.execute_input": "2025-03-12T11:12:03.781036Z",
          "iopub.status.idle": "2025-03-12T11:12:03.788939Z",
          "shell.execute_reply.started": "2025-03-12T11:12:03.781005Z",
          "shell.execute_reply": "2025-03-12T11:12:03.788193Z"
        },
        "id": "VbkNjXgv5A-H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Attention, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Assume VOCAB_SIZE is defined based on your tokenizer\n",
        "VOCAB_SIZE = 10000 #len(tokenizer.word_index) + 1\n",
        "MAX_LENGTH = 19  # as used for encoder inputs\n",
        "DECODER_LENGTH = MAX_LENGTH - 1  # decoder expects sequences without the last token\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(MAX_LENGTH,), name='encoder_inputs')\n",
        "enc_emb = Embedding(VOCAB_SIZE, 256, name='encoder_embedding')(encoder_inputs)\n",
        "encoder_gru = GRU(256, return_sequences=True, return_state=True,\n",
        "                  dropout=0.0, recurrent_dropout=0.0, unroll=True,\n",
        "                  name='encoder_gru')\n",
        "encoder_outputs, encoder_state = encoder_gru(enc_emb)\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(DECODER_LENGTH,), name='decoder_inputs')\n",
        "dec_emb = Embedding(VOCAB_SIZE, 256, name='decoder_embedding')(decoder_inputs)\n",
        "decoder_gru = GRU(256, return_sequences=True, return_state=True,\n",
        "                  dropout=0.0, recurrent_dropout=0.0, unroll=True,\n",
        "                  name='decoder_gru')\n",
        "decoder_outputs, _ = decoder_gru(dec_emb, initial_state=encoder_state)\n",
        "\n",
        "# Attention Layer\n",
        "attention_layer = Attention(name='attention_layer')\n",
        "attention_output = attention_layer([decoder_outputs, encoder_outputs])\n",
        "\n",
        "# Concatenate attention output with decoder outputs\n",
        "decoder_concat = Concatenate(axis=-1, name='decoder_concat')([decoder_outputs, attention_output])\n",
        "\n",
        "# Dense output layer\n",
        "decoder_dense = Dense(VOCAB_SIZE, activation='softmax', name='decoder_dense')\n",
        "output = decoder_dense(decoder_concat)\n",
        "\n",
        "# Build and compile the model\n",
        "model = Model([encoder_inputs, decoder_inputs], output)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, clipnorm=1.0)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-12T11:12:03.789684Z",
          "iopub.execute_input": "2025-03-12T11:12:03.789922Z",
          "iopub.status.idle": "2025-03-12T11:12:05.521368Z",
          "shell.execute_reply.started": "2025-03-12T11:12:03.789889Z",
          "shell.execute_reply": "2025-03-12T11:12:05.520471Z"
        },
        "id": "fB06LW945A-H",
        "outputId": "78fd429b-30c9-4c0d-c58b-530bbc64f705"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1mModel: \"functional\"\u001b[0m\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ encoder_inputs            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_inputs            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ encoder_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │      \u001b[38;5;34m2,560,000\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mEmbedding\u001b[0m)               │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │      \u001b[38;5;34m2,560,000\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mEmbedding\u001b[0m)               │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ encoder_gru (\u001b[38;5;33mGRU\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │        \u001b[38;5;34m394,752\u001b[0m │ encoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]           │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_gru (\u001b[38;5;33mGRU\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │        \u001b[38;5;34m394,752\u001b[0m │ decoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]           │                │ encoder_gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ attention_layer           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ decoder_gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n│ (\u001b[38;5;33mAttention\u001b[0m)               │                        │                │ encoder_gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_concat            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ decoder_gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ attention_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_dense (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m10000\u001b[0m)      │      \u001b[38;5;34m5,130,000\u001b[0m │ decoder_concat[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ encoder_inputs            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_inputs            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ encoder_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ encoder_gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">394,752</span> │ encoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]           │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">394,752</span> │ decoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]           │                │ encoder_gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ attention_layer           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)               │                        │                │ encoder_gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_concat            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ attention_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,130,000</span> │ decoder_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,039,504\u001b[0m (42.11 MB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,039,504</span> (42.11 MB)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,039,504\u001b[0m (42.11 MB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,039,504</span> (42.11 MB)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.word_index)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-12T11:12:05.522252Z",
          "iopub.execute_input": "2025-03-12T11:12:05.522525Z",
          "iopub.status.idle": "2025-03-12T11:12:05.527386Z",
          "shell.execute_reply.started": "2025-03-12T11:12:05.522494Z",
          "shell.execute_reply": "2025-03-12T11:12:05.526645Z"
        },
        "id": "gj17yvFz5A-H",
        "outputId": "a68cd05b-0b3e-455f-8b94-24f79a4e7c3d"
      },
      "outputs": [
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "99488"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_data = question_padded\n",
        "decoder_input_data = answer_padded[:, :-1]  # Remove <end> token\n",
        "decoder_output_data = answer_padded[:, 1:]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-12T11:12:05.528164Z",
          "iopub.execute_input": "2025-03-12T11:12:05.52839Z",
          "iopub.status.idle": "2025-03-12T11:12:05.540764Z",
          "shell.execute_reply.started": "2025-03-12T11:12:05.528372Z",
          "shell.execute_reply": "2025-03-12T11:12:05.540085Z"
        },
        "id": "siZ8n8AS5A-H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "invalid_indices = np.where((decoder_output_data < 0) | (decoder_output_data >= VOCAB_SIZE))\n",
        "\n",
        "if len(invalid_indices[0]) > 0:\n",
        "    print(\"Invalid label tokens found!\")\n",
        "    print(\"Locations:\", invalid_indices)\n",
        "    print(\"Values at those locations:\", decoder_output_data[invalid_indices])\n",
        "else:\n",
        "    print(\"All label tokens are in the valid range [0, {}].\".format(VOCAB_SIZE - 1))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-12T11:12:05.54157Z",
          "iopub.execute_input": "2025-03-12T11:12:05.541878Z",
          "iopub.status.idle": "2025-03-12T11:12:05.608089Z",
          "shell.execute_reply.started": "2025-03-12T11:12:05.541848Z",
          "shell.execute_reply": "2025-03-12T11:12:05.607291Z"
        },
        "id": "t3qzRw8q5A-H",
        "outputId": "caa88f3f-ef67-4116-c6fe-f28ffac47943"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "All label tokens are in the valid range [0, 9999].\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare inputs and outputs\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "encoder_input_data = question_padded\n",
        "decoder_input_data = answer_padded[:, :-1]  # Remove <end> token\n",
        "decoder_output_data = answer_padded[:, 1:]   # Remove <start> token\n",
        "\n",
        "# Train the model\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 12\n",
        "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=1, verbose=1)\n",
        "\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_output_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[reduce_lr]\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-12T11:20:00.164058Z",
          "iopub.execute_input": "2025-03-12T11:20:00.164436Z",
          "iopub.status.idle": "2025-03-12T14:55:33.024784Z",
          "shell.execute_reply.started": "2025-03-12T11:20:00.164408Z",
          "shell.execute_reply": "2025-03-12T14:55:33.023995Z"
        },
        "id": "Eec9gyjt5A-I",
        "outputId": "d2eff6ea-f45e-4484-ab06-ad2e53887166"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/12\n\u001b[1m2771/2771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1077s\u001b[0m 389ms/step - accuracy: 0.5626 - loss: 2.8431 - val_accuracy: 0.5829 - val_loss: 2.6700 - learning_rate: 1.0000e-04\nEpoch 2/12\n\u001b[1m2771/2771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1055s\u001b[0m 381ms/step - accuracy: 0.5921 - loss: 2.5715 - val_accuracy: 0.5899 - val_loss: 2.5706 - learning_rate: 1.0000e-04\nEpoch 3/12\n\u001b[1m2771/2771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1065s\u001b[0m 384ms/step - accuracy: 0.5976 - loss: 2.4827 - val_accuracy: 0.5930 - val_loss: 2.5170 - learning_rate: 1.0000e-04\nEpoch 4/12\n\u001b[1m2771/2771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1073s\u001b[0m 387ms/step - accuracy: 0.6008 - loss: 2.4220 - val_accuracy: 0.5949 - val_loss: 2.4803 - learning_rate: 1.0000e-04\nEpoch 5/12\n\u001b[1m2771/2771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1073s\u001b[0m 387ms/step - accuracy: 0.6012 - loss: 2.3883 - val_accuracy: 0.5969 - val_loss: 2.4523 - learning_rate: 1.0000e-04\nEpoch 6/12\n\u001b[1m2771/2771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1079s\u001b[0m 389ms/step - accuracy: 0.6039 - loss: 2.3552 - val_accuracy: 0.5982 - val_loss: 2.4319 - learning_rate: 1.0000e-04\nEpoch 7/12\n\u001b[1m2771/2771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1072s\u001b[0m 387ms/step - accuracy: 0.6065 - loss: 2.3206 - val_accuracy: 0.5997 - val_loss: 2.4163 - learning_rate: 1.0000e-04\nEpoch 8/12\n\u001b[1m2771/2771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1080s\u001b[0m 390ms/step - accuracy: 0.6067 - loss: 2.3057 - val_accuracy: 0.6006 - val_loss: 2.4039 - learning_rate: 1.0000e-04\nEpoch 9/12\n\u001b[1m2771/2771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1080s\u001b[0m 390ms/step - accuracy: 0.6099 - loss: 2.2746 - val_accuracy: 0.6013 - val_loss: 2.3938 - learning_rate: 1.0000e-04\nEpoch 10/12\n\u001b[1m2771/2771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1082s\u001b[0m 390ms/step - accuracy: 0.6109 - loss: 2.2569 - val_accuracy: 0.6019 - val_loss: 2.3870 - learning_rate: 1.0000e-04\nEpoch 11/12\n\u001b[1m2771/2771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1091s\u001b[0m 394ms/step - accuracy: 0.6110 - loss: 2.2453 - val_accuracy: 0.6024 - val_loss: 2.3788 - learning_rate: 1.0000e-04\nEpoch 12/12\n\u001b[1m2771/2771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1105s\u001b[0m 399ms/step - accuracy: 0.6126 - loss: 2.2248 - val_accuracy: 0.6028 - val_loss: 2.3743 - learning_rate: 1.0000e-04\n",
          "output_type": "stream"
        },
        {
          "execution_count": 18,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<keras.src.callbacks.history.History at 0x7c8871aa7ee0>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model in HDF5 format (includes architecture, weights, optimizer state, etc.)\n",
        "model.save(\"seq2seq_model_gru.h5\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-12T14:55:57.035848Z",
          "iopub.execute_input": "2025-03-12T14:55:57.036189Z",
          "iopub.status.idle": "2025-03-12T14:55:57.291895Z",
          "shell.execute_reply.started": "2025-03-12T14:55:57.036149Z",
          "shell.execute_reply": "2025-03-12T14:55:57.29122Z"
        },
        "id": "fCpovZ7k5A-I"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternatively, save in TensorFlow's SavedModel format:\n",
        "tf.saved_model.save(model, \"saved_seq2seq_model_gru\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-12T14:56:27.619703Z",
          "iopub.execute_input": "2025-03-12T14:56:27.620044Z",
          "iopub.status.idle": "2025-03-12T14:56:29.248351Z",
          "shell.execute_reply.started": "2025-03-12T14:56:27.620013Z",
          "shell.execute_reply": "2025-03-12T14:56:29.247616Z"
        },
        "id": "ydz4CCVk5A-I"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "U95WrEXT5A-I"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}