{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodingYodha/QnA-bot-using-GRU/blob/main/QnA_with_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1QMq6PEtVhN",
        "outputId": "bc16ee28-a878-4576-a57e-dca91e5c0636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'QnA-bot-using-GRU' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/CodingYodha/QnA-bot-using-GRU.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHuQB5-yvMac"
      },
      "source": [
        "Downlaoding the Dataset using convokit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Ez5Ua088tjk0"
      },
      "outputs": [],
      "source": [
        "# from convokit import Corpus, download\n",
        "# corpus = Corpus(filename=download(\"movie-corpus\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DbkmjeVnuiNT"
      },
      "outputs": [],
      "source": [
        "# corpus.print_summary_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "17zu1q5-unYU"
      },
      "outputs": [],
      "source": [
        "# corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNxHUnxPvR8L"
      },
      "source": [
        "Converting the Corpus into Pandas dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KT0_ScGuoAA",
        "outputId": "2d9773d0-9a5f-439e-b589-f5ff5feac0f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading movie-corpus to /root/.convokit/saved-corpora/movie-corpus\n",
            "Downloading movie-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip (40.9MB)... Done\n",
            "Number of Speakers: 9035\n",
            "Number of Utterances: 304713\n",
            "Number of Conversations: 83097\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from convokit import Corpus, download\n",
        "\n",
        "# Download and load the corpus\n",
        "corpus = Corpus(filename=download(\"movie-corpus\"))\n",
        "corpus.print_summary_stats()\n",
        "\n",
        "# Extract utterance data\n",
        "data = []\n",
        "for utt in corpus.iter_utterances():\n",
        "    data.append({\n",
        "        \"conversation_id\": utt.conversation_id,\n",
        "        \"utterance_id\": utt.id,\n",
        "        \"speaker\": utt.speaker.id,\n",
        "        \"text\": utt.text\n",
        "    })\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ChXFviAwgBc"
      },
      "source": [
        "data is in the json format , we need to convert it into model trainable format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KK9khhRJwnN7",
        "outputId": "d9bfbe18-5273-4ba8-b25e-283e336b3ebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What do you think?\n",
            "Answer: Oh, I thought you might have a date  I don't know why I'm bothering to ask, but are you going to Bogey Lowenstein's party Saturday night?\n"
          ]
        }
      ],
      "source": [
        "# Group utterances by conversation_id\n",
        "from collections import defaultdict\n",
        "\n",
        "conversations = defaultdict(list)\n",
        "for entry in data:\n",
        "    conversations[entry['conversation_id']].append((entry['utterance_id'], entry['speaker'], entry['text']))\n",
        "\n",
        "# Extract question-answer pairs\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "for conv_id, utterances in conversations.items():\n",
        "    for i in range(len(utterances) - 1):\n",
        "        current_utterance = utterances[i][2]  # Current utterance text\n",
        "        next_utterance = utterances[i + 1][2]  # Next utterance text\n",
        "        questions.append(current_utterance)\n",
        "        answers.append(next_utterance)\n",
        "\n",
        "# Example output\n",
        "print(\"Question:\", questions[100])\n",
        "print(\"Answer:\", answers[100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyhxKmG4xC8z"
      },
      "source": [
        "Cleaning the text<br>\n",
        "Removing unnecessary characters and normalize it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "IiO_b1JnxUPa"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "X6rZ9-Gyvkxj"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"\\'s\", \" is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z?.!]+\", \" \", text)\n",
        "    text = text.strip()\n",
        "    return text.lower()\n",
        "\n",
        "questions = [clean_text(q) for q in questions]\n",
        "answers = [clean_text(a) for a in answers]\n",
        "\n",
        "# Add <start> and <end> tokens to answers\n",
        "answers = ['<start> ' + a + ' <end>' for a in answers]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7sJux_PxYvj"
      },
      "source": [
        "Tokenization & Padding<br> Tokenizing is for Text and Padding is for Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "dhVrAkVIxYBs"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEdbOOWH0Xib"
      },
      "source": [
        "tokenization : converting text to numerical representation\n",
        "<br>\n",
        "oov_token=\"OOV\" this statement makes_sure that the words which are not in the vocabulary of <br> tensorflow are not ignored but are replaced with <OOV tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "3uYcLihbxxLr"
      },
      "outputs": [],
      "source": [
        "# Tokenizer\n",
        "tokenizer = Tokenizer(filters=\"\", oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(questions + answers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWqo21Sf1CUs"
      },
      "source": [
        "tokenizer.word_index: A dictionary where keys are words and values are their respective integer indices.\n",
        "\n",
        "len(tokenizer.word_index): Counts how many unique words are in the vocabulary.\n",
        "\n",
        "+1: Adds 1 to account for indexing starting from 1, leaving room for special tokens like <OOV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ztU9Mb8lzbqG"
      },
      "outputs": [],
      "source": [
        "# Vocabulary size\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "SgJO9mREziC8"
      },
      "outputs": [],
      "source": [
        "# Convert text to sequences\n",
        "question_sequences = tokenizer.texts_to_sequences(questions)\n",
        "answer_sequences = tokenizer.texts_to_sequences(answers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLqiX-Zw1JPD"
      },
      "source": [
        "texts_to_sequences: Converts each text into a sequence of integers based on the tokenizer's word index.\n",
        "\n",
        "If a word exists in the vocabulary, it's replaced by its index.\n",
        "\n",
        "If a word does not exist, it is replaced with the <OOV token's index.\n",
        "\n",
        "For example:\n",
        "\n",
        "Input: \"How are you?\"\n",
        "\n",
        "Output: [12, 4, 7] (depending on the assigned indices)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "8INaQaB9zqET"
      },
      "outputs": [],
      "source": [
        "# Pad sequences\n",
        "MAX_LENGTH = 19  # Adjust based on your data\n",
        "question_padded = pad_sequences(question_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
        "answer_padded = pad_sequences(answer_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1uSqQY61YHg"
      },
      "source": [
        "pad_sequences: Ensures all sequences have the same length by adding padding or truncating.\n",
        "\n",
        "maxlen=MAX_LENGTH: Specifies the maximum length of the sequences (e.g., 20). Longer sequences are truncated, and shorter ones are padded.\n",
        "\n",
        "padding='post': Adds padding (e.g., zeros) to the end of the sequence.\n",
        "\n",
        "truncating='post': Truncates longer sequences from the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-kZgq_lN1Yvn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDAPn_4O1a80"
      },
      "source": [
        "## Building GRU Model (Seq2Seq with Attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "9iwAPyrg1kJ9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import GRU, Dense, Embedding, Input, Attention\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "9cdk2z9J1geS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "outputId": "43ba90b2-0915-40de-bc7f-3c03cc2d94b3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_inputs            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │     \u001b[38;5;34m25,469,184\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │     \u001b[38;5;34m25,469,184\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder_gru (\u001b[38;5;33mGRU\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │        \u001b[38;5;34m394,752\u001b[0m │ encoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]           │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_gru (\u001b[38;5;33mGRU\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │        \u001b[38;5;34m394,752\u001b[0m │ decoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]           │                │ encoder_gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ attention_layer           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ decoder_gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│ (\u001b[38;5;33mAttention\u001b[0m)               │                        │                │ encoder_gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_concat            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ decoder_gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ attention_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_dense (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m99489\u001b[0m)      │     \u001b[38;5;34m51,037,857\u001b[0m │ decoder_concat[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_inputs            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">25,469,184</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">25,469,184</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder_gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">394,752</span> │ encoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]           │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">394,752</span> │ decoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]           │                │ encoder_gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ attention_layer           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)               │                        │                │ encoder_gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_concat            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ attention_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99489</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">51,037,857</span> │ decoder_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m102,765,729\u001b[0m (392.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">102,765,729</span> (392.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m102,765,729\u001b[0m (392.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">102,765,729</span> (392.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Attention, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define constants\n",
        "VOCAB_SIZE = VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "MAX_LENGTH = 19    # Adjust as needed\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(MAX_LENGTH,), name='encoder_inputs')\n",
        "enc_emb = Embedding(VOCAB_SIZE, 256, name='encoder_embedding')(encoder_inputs)\n",
        "# Set return_sequences=True so we get a 3D tensor for Attention\n",
        "encoder_gru = GRU(256, return_sequences=True, return_state=True, name='encoder_gru')\n",
        "encoder_outputs, encoder_state = encoder_gru(enc_emb)\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(MAX_LENGTH-1,), name='decoder_inputs')\n",
        "dec_emb = Embedding(VOCAB_SIZE, 256, name='decoder_embedding')(decoder_inputs)\n",
        "decoder_gru = GRU(256, return_sequences=True, return_state=True, name='decoder_gru')\n",
        "decoder_outputs, _ = decoder_gru(dec_emb, initial_state=encoder_state)\n",
        "\n",
        "# Attention Layer: compare decoder outputs with full encoder outputs\n",
        "attention_layer = Attention(name='attention_layer')\n",
        "attention_output = attention_layer([decoder_outputs, encoder_outputs])\n",
        "\n",
        "# Concatenate decoder outputs with attention context using the Concatenate layer\n",
        "decoder_concat = Concatenate(axis=-1, name='decoder_concat')([decoder_outputs, attention_output])\n",
        "\n",
        "# Dense output layer\n",
        "decoder_dense = Dense(VOCAB_SIZE, activation='softmax', name='decoder_dense')\n",
        "output = decoder_dense(decoder_concat)\n",
        "\n",
        "# Build and compile the model\n",
        "model = Model([encoder_inputs, decoder_inputs], output)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yi9v87PZ2jvT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5a4257e-2f7d-4366-d4f3-a816aad16d18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# --- Assume you already have question_padded and answer_padded ---\n",
        "# For example, after tokenizing and padding, you have:\n",
        "# question_padded: shape (num_samples, MAX_LENGTH)\n",
        "# answer_padded: shape (num_samples, MAX_LENGTH)\n",
        "# Here, MAX_LENGTH is set to 19 (as per your code).\n",
        "\n",
        "# Prepare decoder data by shifting:\n",
        "decoder_input_data = answer_padded[:, :-1]   # Removes the last token; shape (num_samples, MAX_LENGTH-1)\n",
        "decoder_output_data = answer_padded[:, 1:]     # Removes the first token; shape (num_samples, MAX_LENGTH-1)\n",
        "\n",
        "# Define batch size and number of epochs\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 30\n",
        "\n",
        "# Create a tf.data.Dataset from your numpy arrays\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((question_padded, decoder_input_data), decoder_output_data))\n",
        "# Shuffle, batch, and prefetch to improve efficiency and reduce memory load\n",
        "dataset = dataset.shuffle(buffer_size=10000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Option 1: If you want to train on all data:\n",
        "model.fit(dataset, epochs=EPOCHS)\n",
        "\n",
        "# Option 2: If you want to have a validation split, you can manually split the dataset.\n",
        "# For example, assuming you want an 80/20 split:\n",
        "num_samples = answer_padded.shape[0]\n",
        "train_size = int(0.8 * num_samples)\n",
        "\n",
        "# Convert dataset to a list of batches to allow splitting (only do this if your dataset is not huge)\n",
        "all_batches = list(dataset)\n",
        "num_batches = len(all_batches)\n",
        "train_batches = int(0.8 * num_batches)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(all_batches[:train_batches]).unbatch().batch(BATCH_SIZE)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(all_batches[train_batches:]).unbatch().batch(BATCH_SIZE)\n",
        "\n",
        "model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOBV0/0iWRPIYAsNlWjW2JH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}